{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35470ca1-a065-4ec2-9f04-e6321fc6f81f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event-id</th>\n",
       "      <th>visible</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>location-long</th>\n",
       "      <th>location-lat</th>\n",
       "      <th>external-temperature</th>\n",
       "      <th>gps:hdop</th>\n",
       "      <th>gps:satellite-count</th>\n",
       "      <th>ground-speed</th>\n",
       "      <th>heading</th>\n",
       "      <th>height-above-ellipsoid</th>\n",
       "      <th>sensor-type</th>\n",
       "      <th>individual-taxon-canonical-name</th>\n",
       "      <th>tag-local-identifier</th>\n",
       "      <th>individual-local-identifier</th>\n",
       "      <th>study-name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17714159009</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-06-01 00:01:00.000</td>\n",
       "      <td>28.611073</td>\n",
       "      <td>66.863953</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.666668</td>\n",
       "      <td>323.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>gps</td>\n",
       "      <td>Anser fabalis</td>\n",
       "      <td>191189</td>\n",
       "      <td>X32</td>\n",
       "      <td>Moult migration of taiga bean geese to Novaya ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17714159015</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-06-01 00:11:00.000</td>\n",
       "      <td>28.611586</td>\n",
       "      <td>66.863785</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>139.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>gps</td>\n",
       "      <td>Anser fabalis</td>\n",
       "      <td>191189</td>\n",
       "      <td>X32</td>\n",
       "      <td>Moult migration of taiga bean geese to Novaya ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17714159020</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-06-01 00:21:00.000</td>\n",
       "      <td>28.611605</td>\n",
       "      <td>66.863754</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>192.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>gps</td>\n",
       "      <td>Anser fabalis</td>\n",
       "      <td>191189</td>\n",
       "      <td>X32</td>\n",
       "      <td>Moult migration of taiga bean geese to Novaya ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17714159025</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-06-01 00:31:00.000</td>\n",
       "      <td>28.611860</td>\n",
       "      <td>66.863785</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>265.0</td>\n",
       "      <td>239.0</td>\n",
       "      <td>gps</td>\n",
       "      <td>Anser fabalis</td>\n",
       "      <td>191189</td>\n",
       "      <td>X32</td>\n",
       "      <td>Moult migration of taiga bean geese to Novaya ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17714159031</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-06-01 00:41:00.000</td>\n",
       "      <td>28.611790</td>\n",
       "      <td>66.863716</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>36.0</td>\n",
       "      <td>262.0</td>\n",
       "      <td>gps</td>\n",
       "      <td>Anser fabalis</td>\n",
       "      <td>191189</td>\n",
       "      <td>X32</td>\n",
       "      <td>Moult migration of taiga bean geese to Novaya ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      event-id  visible                timestamp  location-long  location-lat  \\\n",
       "0  17714159009     True  2019-06-01 00:01:00.000      28.611073     66.863953   \n",
       "1  17714159015     True  2019-06-01 00:11:00.000      28.611586     66.863785   \n",
       "2  17714159020     True  2019-06-01 00:21:00.000      28.611605     66.863754   \n",
       "3  17714159025     True  2019-06-01 00:31:00.000      28.611860     66.863785   \n",
       "4  17714159031     True  2019-06-01 00:41:00.000      28.611790     66.863716   \n",
       "\n",
       "   external-temperature  gps:hdop  gps:satellite-count  ground-speed  heading  \\\n",
       "0                   7.0       1.3                  5.0      1.666668    323.0   \n",
       "1                   7.0       1.2                  5.0      0.000000    139.0   \n",
       "2                   8.0       1.2                  5.0      0.277778    192.0   \n",
       "3                   7.0       1.1                  5.0      0.000000    265.0   \n",
       "4                  14.0       1.1                  5.0      0.000000     36.0   \n",
       "\n",
       "   height-above-ellipsoid sensor-type individual-taxon-canonical-name  \\\n",
       "0                   245.0         gps                   Anser fabalis   \n",
       "1                   217.0         gps                   Anser fabalis   \n",
       "2                   232.0         gps                   Anser fabalis   \n",
       "3                   239.0         gps                   Anser fabalis   \n",
       "4                   262.0         gps                   Anser fabalis   \n",
       "\n",
       "   tag-local-identifier individual-local-identifier  \\\n",
       "0                191189                         X32   \n",
       "1                191189                         X32   \n",
       "2                191189                         X32   \n",
       "3                191189                         X32   \n",
       "4                191189                         X32   \n",
       "\n",
       "                                          study-name  \n",
       "0  Moult migration of taiga bean geese to Novaya ...  \n",
       "1  Moult migration of taiga bean geese to Novaya ...  \n",
       "2  Moult migration of taiga bean geese to Novaya ...  \n",
       "3  Moult migration of taiga bean geese to Novaya ...  \n",
       "4  Moult migration of taiga bean geese to Novaya ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Column Names in Bird Tracking Dataset: Index(['event-id', 'visible', 'timestamp', 'location-long', 'location-lat',\n",
      "       'external-temperature', 'gps:hdop', 'gps:satellite-count',\n",
      "       'ground-speed', 'heading', 'height-above-ellipsoid', 'sensor-type',\n",
      "       'individual-taxon-canonical-name', 'tag-local-identifier',\n",
      "       'individual-local-identifier', 'study-name'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading my first dataset (taiga geese)\n",
    "goose_df = pd.read_csv(\"/Users/admin/Downloads/geese.csv\")\n",
    "######## Exploring the dataset(EDA)##########\n",
    "\n",
    "# Displaying first few rows\n",
    "\n",
    "display(goose_df.head())\n",
    "\n",
    "# Printing column names for reference\n",
    "print(\"✅ Column Names in Bird Tracking Dataset:\", goose_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "562d6c10-5399-4b56-aafd-9fe2da032f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Missing Values ===\n",
      "event-id                               0\n",
      "visible                                0\n",
      "timestamp                              0\n",
      "location-long                       6082\n",
      "location-lat                        6082\n",
      "external-temperature               36397\n",
      "gps:hdop                           36397\n",
      "gps:satellite-count                36397\n",
      "ground-speed                       36397\n",
      "heading                            36397\n",
      "height-above-ellipsoid             36397\n",
      "sensor-type                            0\n",
      "individual-taxon-canonical-name        0\n",
      "tag-local-identifier                   0\n",
      "individual-local-identifier            0\n",
      "study-name                             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Checking for missing values in each column\n",
    "print(\"\\n=== Missing Values ===\")\n",
    "print(goose_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f595846-f5f2-4776-ba56-09df96430274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Percentage of Missing Values ===\n",
      "event-id                           0.000000\n",
      "visible                            0.000000\n",
      "timestamp                          0.000000\n",
      "location-long                      0.718663\n",
      "location-lat                       0.718663\n",
      "external-temperature               4.300751\n",
      "gps:hdop                           4.300751\n",
      "gps:satellite-count                4.300751\n",
      "ground-speed                       4.300751\n",
      "heading                            4.300751\n",
      "height-above-ellipsoid             4.300751\n",
      "sensor-type                        0.000000\n",
      "individual-taxon-canonical-name    0.000000\n",
      "tag-local-identifier               0.000000\n",
      "individual-local-identifier        0.000000\n",
      "study-name                         0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculating the percentage of missing values\n",
    "missing_percent = goose_df.isnull().mean() * 100\n",
    "print(\"\\n=== Percentage of Missing Values ===\")\n",
    "print(missing_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f60c599-4f1a-4e2e-ac5a-fff85f4ce608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== After Dropping Rows with Missing GPS Coordinates ===\n",
      "event-id                               0\n",
      "visible                                0\n",
      "timestamp                              0\n",
      "location-long                          0\n",
      "location-lat                           0\n",
      "external-temperature               36272\n",
      "gps:hdop                           36272\n",
      "gps:satellite-count                36272\n",
      "ground-speed                       36272\n",
      "heading                            36272\n",
      "height-above-ellipsoid             36272\n",
      "sensor-type                            0\n",
      "individual-taxon-canonical-name        0\n",
      "tag-local-identifier                   0\n",
      "individual-local-identifier            0\n",
      "study-name                             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Dropping rows with missing essential location data\n",
    "goose_df_clean = goose_df.dropna(subset=[\"location-long\", \"location-lat\"])\n",
    "print(\"\\n=== After Dropping Rows with Missing GPS Coordinates ===\")\n",
    "print(goose_df_clean.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af7367b1-7ebe-41b8-865a-e9c317ba50c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Imputing missing values in sensor columns with the median value.\n",
    "# Defining the list of columns to impute.\n",
    "sensor_columns = [\"external-temperature\", \"gps:hdop\", \"gps:satellite-count\", \"ground-speed\", \"heading\", \"height-above-ellipsoid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb52f834-814d-49ae-b29a-7b72dc82f66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  filling missing values with the column's median.\n",
    "for col in sensor_columns:\n",
    "    if goose_df_clean[col].isnull().sum() > 0:\n",
    "        median_val = goose_df_clean[col].median()\n",
    "        goose_df_clean.loc[:, col] = goose_df_clean[col].fillna(median_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e270a63b-26cb-4607-a0fb-8faebf63a95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Missing Values in Sensor Columns After Imputation ===\n",
      "external-temperature      0\n",
      "gps:hdop                  0\n",
      "gps:satellite-count       0\n",
      "ground-speed              0\n",
      "heading                   0\n",
      "height-above-ellipsoid    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Verifing that missing values have been imputed in the sensor columns\n",
    "print(\"\\n=== Missing Values in Sensor Columns After Imputation ===\")\n",
    "print(goose_df_clean[sensor_columns].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "071f871a-d27f-463e-b88a-5f9a91761995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Cleaned goose dataset saved as: /Users/admin/Downloads/goose_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# resaving the cleaned dataset\n",
    "clean_file_path = \"/Users/admin/Downloads/goose_cleaned.csv\"\n",
    "goose_df_clean.to_csv(clean_file_path, index=False)\n",
    "print(f\"\\n✅ Cleaned goose dataset saved as: {clean_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f28b634-3d1a-4a75-87e6-1340f427b1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xarray in /opt/anaconda3/lib/python3.12/site-packages (2025.1.2)\n",
      "Requirement already satisfied: netCDF4 in /opt/anaconda3/lib/python3.12/site-packages (1.6.5)\n",
      "Requirement already satisfied: h5netcdf in /opt/anaconda3/lib/python3.12/site-packages (1.5.0)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: packaging>=23.2 in /opt/anaconda3/lib/python3.12/site-packages (from xarray) (24.1)\n",
      "Requirement already satisfied: cftime in /opt/anaconda3/lib/python3.12/site-packages (from netCDF4) (1.6.4.post1)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from netCDF4) (2025.1.31)\n",
      "Requirement already satisfied: h5py in /opt/anaconda3/lib/python3.12/site-packages (from h5netcdf) (3.12.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xarray netCDF4 h5netcdf pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91350b07-ac12-404a-8103-9617589102d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ERA5 Dataset 1 Structure (Precipitation):\n",
      "<xarray.Dataset> Size: 143MB\n",
      "Dimensions:     (valid_time: 2193, latitude: 81, longitude: 201)\n",
      "Coordinates:\n",
      "    number      int64 8B ...\n",
      "  * valid_time  (valid_time) datetime64[ns] 18kB 2019-01-01T03:00:00 ... 2020...\n",
      "  * latitude    (latitude) float64 648B 70.0 69.75 69.5 ... 50.5 50.25 50.0\n",
      "  * longitude   (longitude) float64 2kB -10.0 -9.75 -9.5 ... 39.5 39.75 40.0\n",
      "    expver      (valid_time) <U4 35kB ...\n",
      "Data variables:\n",
      "    tp          (valid_time, latitude, longitude) float32 143MB ...\n",
      "Attributes:\n",
      "    GRIB_centre:             ecmf\n",
      "    GRIB_centreDescription:  European Centre for Medium-Range Weather Forecasts\n",
      "    GRIB_subCentre:          0\n",
      "    Conventions:             CF-1.7\n",
      "    institution:             European Centre for Medium-Range Weather Forecasts\n",
      "    history:                 2025-02-24T00:03 GRIB to CDM+CF via cfgrib-0.9.1...\n",
      "\n",
      "✅ ERA5 Dataset 2 Structure (Wind & Temperature):\n",
      "<xarray.Dataset> Size: 571MB\n",
      "Dimensions:     (valid_time: 2193, latitude: 81, longitude: 201)\n",
      "Coordinates:\n",
      "    number      int64 8B ...\n",
      "  * valid_time  (valid_time) datetime64[ns] 18kB 2019-01-01T03:00:00 ... 2020...\n",
      "  * latitude    (latitude) float64 648B 70.0 69.75 69.5 ... 50.5 50.25 50.0\n",
      "  * longitude   (longitude) float64 2kB -10.0 -9.75 -9.5 ... 39.5 39.75 40.0\n",
      "    expver      (valid_time) <U4 35kB ...\n",
      "Data variables:\n",
      "    u10         (valid_time, latitude, longitude) float32 143MB ...\n",
      "    v10         (valid_time, latitude, longitude) float32 143MB ...\n",
      "    t2m         (valid_time, latitude, longitude) float32 143MB ...\n",
      "    sp          (valid_time, latitude, longitude) float32 143MB ...\n",
      "Attributes:\n",
      "    GRIB_centre:             ecmf\n",
      "    GRIB_centreDescription:  European Centre for Medium-Range Weather Forecasts\n",
      "    GRIB_subCentre:          0\n",
      "    Conventions:             CF-1.7\n",
      "    institution:             European Centre for Medium-Range Weather Forecasts\n",
      "    history:                 2025-02-24T00:03 GRIB to CDM+CF via cfgrib-0.9.1...\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "\n",
    "# Define file paths (update if needed)\n",
    "file1_path = \"/Users/admin/Downloads/data_stream-oper_stepType-accum.nc\"  # Precipitation\n",
    "file2_path = \"/Users/admin/Downloads/data_stream-oper_stepType-instant.nc\"  # Wind & Temperature\n",
    "\n",
    "# Load NetCDF files\n",
    "ds1 = xr.open_dataset(file1_path)  # Dataset 1: Precipitation\n",
    "ds2 = xr.open_dataset(file2_path)  # Dataset 2: Wind & Temperature\n",
    "\n",
    "# Print dataset details\n",
    "print(\"✅ ERA5 Dataset 1 Structure (Precipitation):\")\n",
    "print(ds1)\n",
    "\n",
    "print(\"\\n✅ ERA5 Dataset 2 Structure (Wind & Temperature):\")\n",
    "print(ds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8188b0ea-8631-4c8b-ae95-e527307714d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample of Precipitation Data ===\n",
      "           valid_time  latitude  longitude  number expver        tp\n",
      "0 2019-01-01 03:00:00      70.0     -10.00       0   0001  0.000006\n",
      "1 2019-01-01 03:00:00      70.0      -9.75       0   0001  0.000007\n",
      "2 2019-01-01 03:00:00      70.0      -9.50       0   0001  0.000008\n",
      "3 2019-01-01 03:00:00      70.0      -9.25       0   0001  0.000009\n",
      "4 2019-01-01 03:00:00      70.0      -9.00       0   0001  0.000009\n",
      "\n",
      "=== Sample of Wind & Temperature Data ===\n",
      "           valid_time  latitude  longitude  number expver       u10  \\\n",
      "0 2019-01-01 03:00:00      70.0     -10.00       0   0001  6.236557   \n",
      "1 2019-01-01 03:00:00      70.0      -9.75       0   0001  6.176987   \n",
      "2 2019-01-01 03:00:00      70.0      -9.50       0   0001  6.036362   \n",
      "3 2019-01-01 03:00:00      70.0      -9.25       0   0001  5.930893   \n",
      "4 2019-01-01 03:00:00      70.0      -9.00       0   0001  5.837143   \n",
      "\n",
      "         v10         t2m          sp  \n",
      "0  -9.269714  269.363037  102611.875  \n",
      "1  -9.587097  269.529053  102584.875  \n",
      "2  -9.864441  269.661865  102567.875  \n",
      "3 -10.132019  269.777100  102537.875  \n",
      "4 -10.395691  269.882568  102501.875  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert NetCDF to Pandas DataFrame\n",
    "df1 = ds1.to_dataframe().reset_index()  # Convert precipitation dataset\n",
    "df2 = ds2.to_dataframe().reset_index()  # Convert wind & temperature dataset\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\n=== Sample of Precipitation Data ===\")\n",
    "print(df1.head())\n",
    "\n",
    "print(\"\\n=== Sample of Wind & Temperature Data ===\")\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e272e01-247a-43d9-967a-14113a582387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Data filtered for 2019-2020 period.\n"
     ]
    }
   ],
   "source": [
    "# Convert time column to datetime format\n",
    "df1[\"valid_time\"] = pd.to_datetime(df1[\"valid_time\"])\n",
    "df2[\"valid_time\"] = pd.to_datetime(df2[\"valid_time\"])\n",
    "\n",
    "# Filter for 2019-2020 period\n",
    "df1 = df1[(df1[\"valid_time\"] >= \"2019-01-01\") & (df1[\"valid_time\"] <= \"2020-12-31\")]\n",
    "df2 = df2[(df2[\"valid_time\"] >= \"2019-01-01\") & (df2[\"valid_time\"] <= \"2020-12-31\")]\n",
    "\n",
    "print(\"\\n✅ Data filtered for 2019-2020 period.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "934e75ce-a6a5-4c09-89e1-bedf30808b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Merged ERA5 dataset structure:\n",
      "           valid_time  latitude  longitude  number_x expver_x        tp  \\\n",
      "0 2019-01-01 03:00:00      70.0     -10.00         0     0001  0.000006   \n",
      "1 2019-01-01 03:00:00      70.0      -9.75         0     0001  0.000007   \n",
      "2 2019-01-01 03:00:00      70.0      -9.50         0     0001  0.000008   \n",
      "3 2019-01-01 03:00:00      70.0      -9.25         0     0001  0.000009   \n",
      "4 2019-01-01 03:00:00      70.0      -9.00         0     0001  0.000009   \n",
      "\n",
      "   number_y expver_y       u10        v10         t2m          sp  \n",
      "0         0     0001  6.236557  -9.269714  269.363037  102611.875  \n",
      "1         0     0001  6.176987  -9.587097  269.529053  102584.875  \n",
      "2         0     0001  6.036362  -9.864441  269.661865  102567.875  \n",
      "3         0     0001  5.930893 -10.132019  269.777100  102537.875  \n",
      "4         0     0001  5.837143 -10.395691  269.882568  102501.875  \n",
      "\n",
      "✅ ERA5 Merged Dataset Saved: /Users/admin/Downloads/ERA5_Merged_New.csv\n"
     ]
    }
   ],
   "source": [
    "# Merge on timestamp (valid_time), latitude, and longitude\n",
    "era5_merged = pd.merge(df1, df2, on=[\"valid_time\", \"latitude\", \"longitude\"], how=\"inner\")\n",
    "\n",
    "print(\"\\n✅ Merged ERA5 dataset structure:\")\n",
    "print(era5_merged.head())\n",
    "\n",
    "# Save the merged dataset with a new name to avoid overwriting\n",
    "era5_merged.to_csv(\"/Users/admin/Downloads/ERA5_Merged_New.csv\", index=False)\n",
    "print(\"\\n✅ ERA5 Merged Dataset Saved: /Users/admin/Downloads/ERA5_Merged_New.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a8b3572-0745-422a-9317-8297bea19583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Missing values per column:\n",
      "valid_time    0\n",
      "latitude      0\n",
      "longitude     0\n",
      "number_x      0\n",
      "expver_x      0\n",
      "tp            0\n",
      "number_y      0\n",
      "expver_y      0\n",
      "u10           0\n",
      "v10           0\n",
      "t2m           0\n",
      "sp            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the merged ERA5 dataset\n",
    "era5_file_path = \"/Users/admin/Downloads/ERA5_Merged_New.csv\"\n",
    "era5_df = pd.read_csv(era5_file_path)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n✅ Missing values per column:\")\n",
    "print(era5_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9165d89-e6cb-4c6d-931b-2bc7e30c4dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Unnecessary columns removed.\n"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary columns\n",
    "era5_df = era5_df.drop(columns=[\"number\", \"expver\"], errors=\"ignore\")\n",
    "\n",
    "print(\"\\n✅ Unnecessary columns removed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdb2ed12-4df6-49aa-87ed-c54618ec22d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Timestamp format verified.\n"
     ]
    }
   ],
   "source": [
    "# Convert valid_time to datetime format\n",
    "era5_df[\"valid_time\"] = pd.to_datetime(era5_df[\"valid_time\"])\n",
    "\n",
    "print(\"\\n✅ Timestamp format verified.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b945b99-f20a-41e0-a5c0-b0db1073d239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Cleaned ERA5 Dataset Saved: /Users/admin/Downloads/ERA5_Cleaned_Final.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the cleaned dataset\n",
    "era5_df.to_csv(\"/Users/admin/Downloads/ERA5_Cleaned_Final.csv\", index=False)\n",
    "\n",
    "print(\"\\n✅ Cleaned ERA5 Dataset Saved: /Users/admin/Downloads/ERA5_Cleaned_Final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21577e5c-b8d6-4000-95c4-246cf5fddde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Total rows in dataset: 35655390\n",
      "\n",
      "✅ Available columns in dataset:\n",
      "Index(['valid_time', 'latitude', 'longitude', 'number_x', 'expver_x', 'tp',\n",
      "       'number_y', 'expver_y', 'u10', 'v10', 't2m', 'sp'],\n",
      "      dtype='object')\n",
      "\n",
      "✅ Sample data:\n",
      "            valid_time  latitude  longitude  number_x  expver_x        tp  \\\n",
      "0  2019-01-01 03:00:00      70.0     -10.00         0         1  0.000006   \n",
      "1  2019-01-01 03:00:00      70.0      -9.75         0         1  0.000007   \n",
      "2  2019-01-01 03:00:00      70.0      -9.50         0         1  0.000008   \n",
      "3  2019-01-01 03:00:00      70.0      -9.25         0         1  0.000009   \n",
      "4  2019-01-01 03:00:00      70.0      -9.00         0         1  0.000009   \n",
      "\n",
      "   number_y  expver_y       u10        v10        t2m          sp  \n",
      "0         0         1  6.236557  -9.269714  269.36304  102611.875  \n",
      "1         0         1  6.176987  -9.587097  269.52905  102584.875  \n",
      "2         0         1  6.036362  -9.864441  269.66187  102567.875  \n",
      "3         0         1  5.930893 -10.132019  269.77710  102537.875  \n",
      "4         0         1  5.837143 -10.395691  269.88257  102501.875  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the large ERA5 dataset\n",
    "era5_file_path = \"/Users/admin/Downloads/ERA5_Cleaned_Final.csv\"\n",
    "era5_df = pd.read_csv(era5_file_path)\n",
    "\n",
    "# Check dataset size and structure\n",
    "print(f\"\\n✅ Total rows in dataset: {era5_df.shape[0]}\")\n",
    "print(\"\\n✅ Available columns in dataset:\")\n",
    "print(era5_df.columns)\n",
    "\n",
    "# Show first few rows\n",
    "print(\"\\n✅ Sample data:\")\n",
    "print(era5_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62399e33-0910-4402-9316-0409c4ce767a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Unnecessary metadata columns removed (if any).\n"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary columns (Only if they exist)\n",
    "columns_to_remove = [\"number\", \"expver\"]  # Remove these if present\n",
    "era5_df = era5_df.drop(columns=[col for col in columns_to_remove if col in era5_df.columns], errors=\"ignore\")\n",
    "\n",
    "print(\"\\n✅ Unnecessary metadata columns removed (if any).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b311d8e-ceae-4294-b423-2b1deeeb1fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Reduced spatial redundancy. New dataset size: 35655390 rows\n"
     ]
    }
   ],
   "source": [
    "# Round latitude and longitude to 1 decimal place (groups nearby locations)\n",
    "era5_df[\"latitude\"] = era5_df[\"latitude\"].round(1)\n",
    "era5_df[\"longitude\"] = era5_df[\"longitude\"].round(1)\n",
    "\n",
    "# Drop duplicates to keep only one value per location per time\n",
    "era5_df = era5_df.drop_duplicates(subset=[\"valid_time\", \"latitude\", \"longitude\"])\n",
    "\n",
    "print(f\"\\n✅ Reduced spatial redundancy. New dataset size: {era5_df.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22a665bf-71dc-4b8a-b546-d3e6e2866145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Timestamp format verified.\n"
     ]
    }
   ],
   "source": [
    "# Convert valid_time to datetime format\n",
    "era5_df[\"valid_time\"] = pd.to_datetime(era5_df[\"valid_time\"])\n",
    "\n",
    "print(\"\\n✅ Timestamp format verified.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a72e6af5-e72f-4ffa-ad02-ca1d1d97af89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Reduced dataset size after grouping by daily means: 11885130 rows\n"
     ]
    }
   ],
   "source": [
    "# Convert valid_time to just the date (removing hour precision)\n",
    "era5_df[\"date\"] = era5_df[\"valid_time\"].dt.date\n",
    "\n",
    "# Compute daily averages for each location\n",
    "era5_df = era5_df.groupby([\"date\", \"latitude\", \"longitude\"]).mean().reset_index()\n",
    "\n",
    "# Convert date column back to datetime format\n",
    "era5_df[\"date\"] = pd.to_datetime(era5_df[\"date\"])\n",
    "\n",
    "# Rename \"date\" back to \"valid_time\" for consistency\n",
    "era5_df = era5_df.rename(columns={\"date\": \"valid_time\"})\n",
    "\n",
    "print(f\"\\n✅ Reduced dataset size after grouping by daily means: {era5_df.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71d615b3-f67b-492b-b540-a19571829cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Final Optimized ERA5 Dataset Saved: /Users/admin/Downloads/ERA5_Optimized.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the optimized dataset\n",
    "optimized_file_path = \"/Users/admin/Downloads/ERA5_Optimized.csv\"\n",
    "era5_df.to_csv(optimized_file_path, index=False)\n",
    "\n",
    "print(f\"\\n✅ Final Optimized ERA5 Dataset Saved: {optimized_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d558e33-2526-4558-b1a6-900b302ad690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Total rows in dataset: 11885130\n",
      "\n",
      "✅ File size estimate: 2436.93 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the optimized ERA5 dataset\n",
    "era5_file_path = \"/Users/admin/Downloads/ERA5_Optimized.csv\"\n",
    "era5_df = pd.read_csv(era5_file_path)\n",
    "\n",
    "# Check total rows and file size\n",
    "print(f\"\\n✅ Total rows in dataset: {era5_df.shape[0]}\")\n",
    "print(f\"\\n✅ File size estimate: {era5_df.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6560425-6c2f-470e-be3e-498eafb9ea35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Taiga Goose Data Sample:\n",
      "      event-id  visible           timestamp  location-long  location-lat  \\\n",
      "0  17714159009     True 2019-06-01 00:01:00      28.611073     66.863953   \n",
      "1  17714159015     True 2019-06-01 00:11:00      28.611586     66.863785   \n",
      "2  17714159020     True 2019-06-01 00:21:00      28.611605     66.863754   \n",
      "3  17714159025     True 2019-06-01 00:31:00      28.611860     66.863785   \n",
      "4  17714159031     True 2019-06-01 00:41:00      28.611790     66.863716   \n",
      "\n",
      "   external-temperature  gps:hdop  gps:satellite-count  ground-speed  heading  \\\n",
      "0                   7.0       1.3                  5.0      1.666668    323.0   \n",
      "1                   7.0       1.2                  5.0      0.000000    139.0   \n",
      "2                   8.0       1.2                  5.0      0.277778    192.0   \n",
      "3                   7.0       1.1                  5.0      0.000000    265.0   \n",
      "4                  14.0       1.1                  5.0      0.000000     36.0   \n",
      "\n",
      "   height-above-ellipsoid sensor-type individual-taxon-canonical-name  \\\n",
      "0                   245.0         gps                   Anser fabalis   \n",
      "1                   217.0         gps                   Anser fabalis   \n",
      "2                   232.0         gps                   Anser fabalis   \n",
      "3                   239.0         gps                   Anser fabalis   \n",
      "4                   262.0         gps                   Anser fabalis   \n",
      "\n",
      "   tag-local-identifier individual-local-identifier  \\\n",
      "0                191189                         X32   \n",
      "1                191189                         X32   \n",
      "2                191189                         X32   \n",
      "3                191189                         X32   \n",
      "4                191189                         X32   \n",
      "\n",
      "                                          study-name  \n",
      "0  Moult migration of taiga bean geese to Novaya ...  \n",
      "1  Moult migration of taiga bean geese to Novaya ...  \n",
      "2  Moult migration of taiga bean geese to Novaya ...  \n",
      "3  Moult migration of taiga bean geese to Novaya ...  \n",
      "4  Moult migration of taiga bean geese to Novaya ...  \n",
      "\n",
      "✅ ERA5 Data Sample:\n",
      "  valid_time  latitude  longitude         valid_time.1  number_x  expver_x  \\\n",
      "0 2019-01-01      50.0      -10.0  2019-01-01 10:00:00       0.0       1.0   \n",
      "1 2019-01-01      50.0       -9.8  2019-01-01 10:00:00       0.0       1.0   \n",
      "2 2019-01-01      50.0       -9.5  2019-01-01 10:00:00       0.0       1.0   \n",
      "3 2019-01-01      50.0       -9.2  2019-01-01 10:00:00       0.0       1.0   \n",
      "4 2019-01-01      50.0       -9.0  2019-01-01 10:00:00       0.0       1.0   \n",
      "\n",
      "         tp  number_y  expver_y       u10       v10         t2m             sp  \n",
      "0  0.000018       0.0       1.0 -3.606949  2.477310  284.134113  103781.313333  \n",
      "1  0.000016       0.0       1.0 -3.452978  2.175227  284.022787  103802.646667  \n",
      "2  0.000014       0.0       1.0 -3.114110  1.853938  283.928380  103817.646667  \n",
      "3  0.000014       0.0       1.0 -2.766129  1.544367  283.862633  103820.313333  \n",
      "4  0.000012       0.0       1.0 -2.399918  1.254979  283.866537  103836.980000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load Taiga Goose dataset\n",
    "goose_file_path = \"/Users/admin/Downloads/goose_cleaned.csv\"\n",
    "goose_df = pd.read_csv(goose_file_path)\n",
    "\n",
    "# Load Optimized ERA5 dataset\n",
    "era5_file_path = \"/Users/admin/Downloads/ERA5_Optimized.csv\"\n",
    "era5_df = pd.read_csv(era5_file_path)\n",
    "\n",
    "# Convert timestamps to datetime format\n",
    "goose_df[\"timestamp\"] = pd.to_datetime(goose_df[\"timestamp\"])\n",
    "era5_df[\"valid_time\"] = pd.to_datetime(era5_df[\"valid_time\"])\n",
    "\n",
    "# Display structure\n",
    "print(\"\\n✅ Taiga Goose Data Sample:\")\n",
    "print(goose_df.head())\n",
    "\n",
    "print(\"\\n✅ ERA5 Data Sample:\")\n",
    "print(era5_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3decab51-0c65-4175-ab33-b54c8e12aa1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Merged dataset structure:\n",
      "      event-id  visible           timestamp  location-long  location-lat  \\\n",
      "0  17714159009     True 2019-06-01 00:01:00      28.611073     66.863953   \n",
      "1  17714159010     True 2019-06-01 00:02:00      28.559181     67.096291   \n",
      "2  17714159011     True 2019-06-01 00:03:00      29.482441     66.555397   \n",
      "3  17714159012     True 2019-06-01 00:04:00      30.416782     63.318619   \n",
      "4  17714159014     True 2019-06-01 00:08:00      26.962051     65.176041   \n",
      "\n",
      "   external-temperature  gps:hdop  gps:satellite-count  ground-speed  heading  \\\n",
      "0                   7.0       1.3                  5.0      1.666668    323.0   \n",
      "1                   7.0       0.9                  8.0      0.277778    143.0   \n",
      "2                   5.0       0.8                  9.0      0.000000     86.0   \n",
      "3                   9.0       0.8                 10.0      0.000000    198.0   \n",
      "4                   6.0       1.5                  5.0      0.000000    251.0   \n",
      "\n",
      "   ...         valid_time.1 number_x expver_x       tp number_y expver_y  \\\n",
      "0  ...  2019-06-01 10:00:00      0.0      1.0  0.00007      0.0      1.0   \n",
      "1  ...  2019-06-01 10:00:00      0.0      1.0  0.00007      0.0      1.0   \n",
      "2  ...  2019-06-01 10:00:00      0.0      1.0  0.00007      0.0      1.0   \n",
      "3  ...  2019-06-01 10:00:00      0.0      1.0  0.00007      0.0      1.0   \n",
      "4  ...  2019-06-01 10:00:00      0.0      1.0  0.00007      0.0      1.0   \n",
      "\n",
      "        u10       v10         t2m             sp  \n",
      "0  3.270223  5.753423  285.107663  101093.873333  \n",
      "1  3.270223  5.753423  285.107663  101093.873333  \n",
      "2  3.270223  5.753423  285.107663  101093.873333  \n",
      "3  3.270223  5.753423  285.107663  101093.873333  \n",
      "4  3.270223  5.753423  285.107663  101093.873333  \n",
      "\n",
      "[5 rows x 28 columns]\n",
      "\n",
      "✅ Merged Dataset Saved: /Users/admin/Downloads/Merged_Taiga_Goose_ERA5.csv\n"
     ]
    }
   ],
   "source": [
    "# Sort both datasets by timestamp\n",
    "goose_df = goose_df.sort_values(\"timestamp\")\n",
    "era5_df = era5_df.sort_values(\"valid_time\")\n",
    "\n",
    "# Merge using nearest valid_time (ERA5) to each bird timestamp\n",
    "merged_df = pd.merge_asof(goose_df, era5_df, left_on=\"timestamp\", right_on=\"valid_time\", direction=\"nearest\")\n",
    "\n",
    "# Drop the duplicate valid_time column\n",
    "merged_df = merged_df.drop(columns=[\"valid_time\"])\n",
    "\n",
    "print(\"\\n✅ Merged dataset structure:\")\n",
    "print(merged_df.head())\n",
    "\n",
    "# Save the merged dataset\n",
    "merged_file_path = \"/Users/admin/Downloads/Merged_Taiga_Goose_ERA5.csv\"\n",
    "merged_df.to_csv(merged_file_path, index=False)\n",
    "\n",
    "print(f\"\\n✅ Merged Dataset Saved: {merged_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff20a67-1fe5-43ff-a6a7-ad339cec6011",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
